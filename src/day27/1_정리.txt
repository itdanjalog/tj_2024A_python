CNTK, Theano, MXNet는 모두 딥러닝 프레임워크로, 머신러닝 및 딥러닝 모델을 구현하고 학습하는 데 사용됩니다

- CNTK (Microsoft Cognitive Toolkit) / 씨엔티케이
- Theano / 테아노
- MXNet/ 엠엑스넷

[ 하이퍼 파라미터 ]
    - 파라미터 : 매개변수
    - 머신러닝 및 딥러닝 모델의 학습 과정에서 설정되는 값
    - 예시
        학습률- 경사하강법과 같은 최적화 알고리즘에서 가중치를 업데이트할 때 얼마나 큰 스텝을 취할지를 결정합니다. 너무 크면 최적점에 도달하지 못하고, 너무 작으면 학습 속도가 느려질 수 있습니다.
        에포크 수-전체 훈련 데이터셋을 몇 번 반복하여 모델을 학습할지를 설정합니다. 에포크 수가 너무 적으면 과소적합(overfitting)될 수 있고, 너무 많으면 과적합(overfitting)될 수 있습니다.
        배치 크기-모델 학습 시 한 번에 사용하는 샘플의 수를 지정합니다. 배치 크기가 크면 더 빠른 학습이 가능하지만 메모리 요구량이 커지고, 작으면 더 많은 업데이트를 할 수 있지만 학습 시간이 길어질 수 있습니다.
        모델구조-층의 수, 각 층의 노드 수, 활성화 함수 등과 같은 모델 아키텍처에 대한 설정입니다. 예를 들어, 깊이 있는 신경망을 설계할 때 각 층의 노드 수와 층의 개수를 선택하는 것도 하이퍼파라미터입니다.
        정규화방법 - 과적합을 방지하기 위해 사용하는 정규화 기법에 대한 설정입니다. 예를 들어, 드롭아웃(dropout) 비율이나 L1/L2 정규화의 강도를 설정할 수 있습니다.
        옵티마이저-모델 학습에 사용되는 최적화 알고리즘을 선택합니다. 예를 들어, SGD, Adam, RMSprop 등이 있습니다.
    - 튜닝
        - 하이퍼파라미터의 최적 값을 찾는 과정을 **하이퍼파라미터 튜닝(Hyperparameter Tuning)**이라고 합니다. 이 과정은 다양한 조합의 하이퍼파라미터를 실험하고, 모델의 성능을 평가하여 최적의 조합을 찾는 것입니다.

[ 과소적합 vs 과대적합 ]
    - 과소적합 :
        모델이 너무 단순함 (예: 너무 적은 수의 매개변수, 낮은 차원의 모델 사용)
        데이터 부족 또는 데이터 품질 문제
        충분한 학습이 이루어지지 않음
    -과대적합
        모델이 너무 복잡함 (예: 너무 많은 층, 노드 또는 매개변수)
        훈련 데이터가 부족하거나 다양성이 부족함

    과소적합: 모델이 학습 데이터의 패턴을 제대로 학습하지 못하고, 일반화 능력이 떨어지는 상태. 주로 모델이 너무 간단할 때 발생.
    과대적합: 모델이 학습 데이터에 지나치게 적합하여 새로운 데이터에 대한 성능이 떨어지는 상태. 주로 모델이 너무 복잡할 때 발생.

[ 에포크 ]
    - 전체 훈련 데이터셋이 모델을 통해 한 번 완전히 통과한 횟수를 나타냅니다. 간단히 말하면, 에포크는 모델이 훈련 데이터를 얼마나 반복적으로 학습하는지를 나타내는 단위입니다.
    - 에포크는 훈련 데이터셋이 모델을 통해 한 번 완전히 통과한 횟수입니다.
    - 에포크 수는 모델의 성능에 중요한 영향을 미치며, 적절한 수를 선택하는 것이 중요합니다.
    - 너무 적은 에포크 수는 과소적합을 초래할 수 있고, 너무 많은 에포크 수는 과적합을 초래할 수 있습니다.
        예를 들어, 1000개의 훈련 데이터가 있고 배치 크기가 100이라면:
            1 에포크 동안 모델은 10개의 배치를 처리하게 됩니다.
            에포크 수가 5라면, 모델은 전체 데이터를 5번 반복적으로 학습합니다.

[손실함수]
    - 손실 함수는 모델의 예측과 실제 간의 차이를 측정하여 성능을 평가하고, 최적화 과정에서 가중치를 조정하는 기준을 제공합니다.
    - 다양한 종류의 손실 함수가 있으며, 문제의 유형에 맞게 적절한 함수를 선택하는 것이 중요합니다.
        - 손실 함수는 문제의 특성에 따라 다르게 선택됩니다. 예를 들어, 회귀 문제에서는 MSE나 MAE를 사용하고, 이진 분류 문제에서는 이진 교차 엔트로피를 사용하는 것이 일반적입니다.

[경사하강법]
    - 손실 함수를 최소화하여 모델의 가중치를 최적화하는 중요한 최적화 알고리즘입니다. 다양한 변형(배치, 미니배치, 확률적 등)을 통해 다양한 상황에 맞춰 사용할 수 있으며, 머신러닝 및 딥러닝의 핵심 원리 중 하나입니다.
    목표: 손실 함수의 값을 최소화하는 가중치(혹은 파라미터)를 찾는 것이 목표입니다. 손실 함수는 모델의 예측값과 실제값 간의 차이를 측정합니다.
    기울기(Gradient): 손실 함수의 기울기는 현재 위치에서 함수의 증가율을 나타냅니다. 경사하강법에서는 손실 함수의 기울기를 계산하여 가중치를 업데이트합니다.
    학습률 (Learning Rate)
학습률은 기울기(gradient)에 곱해져 가중치 업데이트의 크기를 결정합니다. 학습률이 너무 크면 최적점에 도달하지 못하고 발산할 수 있으며, 너무 작으면 학습 속도가 느려지고 지역 최적점에 빠질 수 있습니다.