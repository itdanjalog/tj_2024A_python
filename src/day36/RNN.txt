[순환 신경망]
    - Recurrent Neural Network
    - 컴퓨터가 데이터를 기억하고 , 앞에서 받은 정보를 활용하여 다음에 어떤 결과가 나올지 예측 하는 딥러닝 모델
    - 예] '나는 오늘' 이라고 했을때 다음에 나올 단어를 예측한다면 RNN은 앞에서 받은 정보를 기억하여 '공부' '여행' 같은 단어를 예측한다.
    - CNN(합성곱) 차이
        CNN : 1. 이미지(2차원데이터) 2. 특징(색,모양)찾기 3. 이미지 분석하여 이미지 분류 4. 순서 상관없이 데이터를 입력
        RNN : 1. 텍스트,음성(1차원데이터) 2. 이전 데이터를 기억하여 다음 데이터 예측 3.문장 만들기 4. 순서(시계열/시간) 유지하면서 데이터를 입력

SimpleRNN
    - 가장 기본적인 단일 순환신경망 구조
    - 동작구조 : 계산처리된 데이터를 받아서 다음 계산으로 전달하는 구조

LSTM
    - Long short-Term Memory
    - 중요한 정보는 오래 기억하고 , 덜 중요한 정보를 잊어버리는 방법
    - 사용처 : 많은 데이터 와 데이터를 오래 유지 할때 사용 , 긴 문자열 , 감정분석
    - 동작구조 : 게이트3개(망각,입력,출력) 존재한다.
        망각게이트 : Forget Gate : 필요없다고 생각하는 데이터를 잊어버릴지 결정하는 게이트 ( 시그모이드 함수 0:잊어버리고 1:기억한다 )
        입력게이트 : Input Gate : 현재 데이터를 전달받아 계산을 통해 시그모이드 결과에 따라 상태를 더할지 말지 결정하는 게이트
        출력게이트 : Output Gate : 출력 값을 결정 하고 오랫동안 기억하는 데이트

GRU
    - Gated Recurrent Unit
    - LSTM과 마찬가지로 중요한 정보는 오래 기억하고 , 덜 중요한 정보를 잊어버리는 방법
    - LSTM 보다 더 간단한 구조 이며 빠르다.
    - 사용처 : 조금 더 적은 데이터 와 빠른 속도가 필요할때 사용 , 실시간 처리 , 모바일
    - 동작구조 : 게이트2개( 업데이트 , 리셋 ) 존재한다.
        업데이트게이트 : 망각/입력 게이트 모두 제어한다, 새로운 데이터를 얼마나 기억할지 또는 잊을지 결정하는 게이트
        리셋게이트 : 이전 데이터를 얼마나 반영할지 결정하는 게이트

계산식 기호
    h : 현재 시점의 상태 값
    x : 입력값
    W : 현재 시점의 가중치
    U : 이전 출력에 대한 가중치
    t : time step 이며 연속적인 순서
    F : 망각게이트 ( 정보를 기억할지 잊을지 결정 )
    I : 입력게이트 ( 새로운 정보를 더할지 말지 결정 )
    O : 출력게이트 ( 최종 출력 결정 )
        - 시그모이드 함수 ( 1 또는 0 값을 출력해 게이트를 열지 말지 사용되는 함수 )
        - 하이퍼폴릭 탄젠트 ( 데이터를 -1 에서 1 사이 값으로 변환 함수 )
    Z : 업데이트 게이트( 현재 입력을 얼마나 기억할지 결정 )
    R : 리셋 게이트( 이전 데이터를 얼마나 기억할지 결정 )

[ 임베딩 ]
    - 컴퓨터가 글자나 단어를 숫자로 변환 하여 컴퓨터가 이해하고 처리 할 수 있도록 하는 방법
    - 원핫 인코딩 : 현재 단어의 위치를 1로 표현 하고 다른 단어를 0 으로 표현하는 방법
        예] 나는[1,0,0] 딥러닝이[0,1,0] 재미있다[0,0,1]
        - 문제점 : 문장이 길어지면 대부분이 0 으로 채워져서 희소행렬 이라고 한다. 메모리 낭비 와 단어 유사도 알기 어렵다.
    - 밀집행렬 : 0 이 아닌 실수값으로 채워서 표현 , 메모리 낭비 적게 , 단어 유사도 알수 있다. ( 케라스API )
    - 임베딩 레이어

[ 양방향 RNN ]
    - 데이터를 앞에서부터 읽을 뿐만 아니라 뒤에서부터도 읽는 양방향 RNN신경망
    - 순차적인 RNN은 앞에 있는 데이터를 보고 예측하지만 양방향 RNN은 앞뒤 데이터를 보고 예측한다.
    - 예] "나는 밥을 먹었다" 라는 문장을 학습할때 '나는' 과 뒤 에있는 '먹었다' 를 학습하면서 '밥' 이라는 단어를 예측할수 있다.
[ 유닛 ]
    - CNN( 뉴런 ) 과 비슷한 역할 한다. 뉴런 == 유닛
    - LSTM 레이어는 여러개의 유닛(뉴런)으로 구성되고 각 유닛(뉴런)은 입ㄹ력을 처리하고 출력하는 역할
    - 즉] 32개 라고하면 32개의 독립적인 유닛(뉴런)을 통해 각 데이터를 처리하고 패턴을 학습한다.
        - 뉴런이 많으면 좋은건가? 많으면 더 많은 학습과 표현이 향상 되지만 과대적합이 될수 있다. 적절하게 사용한다. 128 64 32 주로 사용된다.
[ 스태킹 RNN ]
    - 여러 RNN 레이어 으로 쌓인 순환신경망 구조이다. # 복잡한 패턴을 학습할수 있다.
    - 기본적으로 RNN은 마지막 상태 값만 출력하기 때문에 앞에 출력상태는 무시하게 된다.
    - return_sequences 속성을 true 로 설정하여 레이어를 쌓을수 있다.
[ 순환 드롭아웃 ]
    - 드롭아웃 : 훈련 과정 중에 일부 뉴런(유닛)을 무작위로 선택하여 0.2 이면 20%를 무작위로 비활성화하여 학습을 하지 않는 뉴런(유닛) 구성
    - 목적 : 과대적합을 방지하고 무작위를 이용한 다양한 뉴런조합의 학습을 하여 일반화된 성능을 향상 할수 있다.
    - 순환 드롭아웃 : RNN의 드롭아웃을 적용한다.
        - recurrent_dropout 속성 : 순환 연결된 데이터 이며 , 이전 타임스탭의 출력된 데이터에 대해 무작위로 비활성화 한다.
        - dropout 속성 : 입력 (새로운) 데이터 적용되며, 전체 입력 유닛(뉴런)에 무작위로 비활성화 한다.


















