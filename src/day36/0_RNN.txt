[순환신경망]
    - Recurrent Neural Network
    - 컴퓨터가 데이터를 기억하고, 앞에서 받은 정보를 활용해서 다음에 어떤 결과가 나올지 예측할 수 있도록 돕는 딥러닝 모델이야.
    - 예]
    예를 들어, 너가 친구한테 문자를 보내려고 "나는 오늘"이라고 썼을 때,
    다음에 나올 단어를 예측한다고 생각해봐. 아마 "피자를"이나 "공부를" 같은 단어를 예상할 수 있을 거야.
    이처럼 RNN은 앞에서 받은 정보(예: "나는 오늘")를 기억해서 그다음에 나올 것을 잘 예측하도록 훈련할 수 있어.
* 시계열 데이터(Time Series Data)는 시간에 따라 변화하는 데이터를 모아 놓은 것이야.

[ CNN 과 RNN 차이 ]
CNN (합성곱 신경망)
주로 사용되는 데이터: 이미지와 같은 2차원 데이터에 적합해.
작동 방식: 이미지를 작은 부분으로 나눠서 중요한 특징(예: 색, 모양)을 찾아내는 데 집중해. 이렇게 하면 이미지 속에서 어떤 물체가 있는지 잘 알아낼 수 있어.
예시: 고양이 사진이 들어오면, CNN은 고양이의 귀, 눈, 털 같은 부분을 분석해서 "이건 고양이야!"라고 판단할 수 있어.
RNN (순환 신경망)
주로 사용되는 데이터: 텍스트, 음성, 시간에 따라 변화하는 1차원 데이터에 적합해.
작동 방식: 이전 데이터(예: 문장이나 시계열 데이터)를 기억하고 그 정보를 바탕으로 다음에 나올 것을 예측해. 시간의 흐름이 중요해!
예시: "나는 오늘"이라는 문장이 주어지면, RNN은 다음에 올 단어를 "아이스크림을"이나 "학교에" 같은 단어로 예측할 수 있어.
++++
CNN은 이미지를 처리하는 데 강하고, RNN은 시간 순서가 중요한 데이터를 처리하는 데 강해. 각 모델은 서로 다른 종류의 문제를 해결하기 위해 설계되었어!


[SimpleRNN]

[LSTM]
    - Long Short-Term Memo
    - SimpleRNN처럼 앞에서 받은 정보를 기억하지만, 어떤 정보는 금방 잊어버리기도 해.
    그런데 LSTM은 중요한 정보는 오래 기억하고, 덜 중요한 정보는 잊어버리는 방법을 배워!
    예를 들어, 책을 읽을 때 중요한 내용은 오래 기억하고, 덜 중요한 내용은 금방 잊는 것과 비슷해.

[GRU]
    - Gated Recurrent Unit
    - GRU는 LSTM처럼 중요한 정보를 오래 기억하고 덜 중요한 건 잊어버리는 기능이 있지만,
    조금 더 간단해. 마치 네가 중요한 일을 오래 기억하고 덜 중요한 일은 빠르게 잊어버릴 수 있도록 도와주는 도구 같아

[알고리즘 공통]
순차 데이터를 처리: 세 가지 모델 모두 시간 순서가 중요한 데이터를 처리하는 데 사용돼. 예를 들어, 텍스트, 음성, 주식 데이터처럼 앞뒤가 연결된 데이터에 적합해.
기억 기능: 앞에서 받은 데이터를 기억하고 그걸 바탕으로 다음 데이터를 예측할 수 있어.
순환구조: 각 단계에서 받은 데이터를 다음 단계로 전달하면서 정보를 갱신해 나가는 순환구조를 가지고 있어.
딥러닝 네트워크: 모두 딥러닝의 한 종류로, 데이터를 학습해서 패턴을 찾고 예측을 할 수 있어.
-
구조: 가장 기본적인 순환 신경망 구조.
기억력: 시간이 지날수록 정보를 금방 잊어버리기 때문에, 긴 데이터를 처리할 때는 성능이 떨어져.
속도: 계산이 가장 빠르고 간단해.
사용 예시: 짧은 데이터나 간단한 문제(예: 간단한 시계열 예측).
-
구조: 복잡한 메모리 구조를 갖고 있어서 중요한 정보는 오래 기억하고, 덜 중요한 건 잊을 수 있어.
기억력: 긴 데이터를 처리하는 데 적합하고, 과거의 중요한 정보를 오래 유지할 수 있어.
속도: 구조가 복잡해서 계산 속도는 SimpleRNN보다 느려.
사용 예시: 장기 기억이 중요한 문제(예: 긴 문장 번역, 긴 시계열 예측, 감정 분석).
-
구조: LSTM보다 조금 더 단순한 구조로, 중요한 정보는 기억하고 덜 중요한 건 잊는 게이트를 가지고 있어.
기억력: LSTM과 비슷한 성능을 내면서도 더 간단해서, 적당히 긴 데이터를 처리할 수 있어.
속도: LSTM보다 빠르고 SimpleRNN보다는 느리지만, 복잡한 문제를 더 잘 해결해.
사용 예시: LSTM과 비슷하지만 더 가벼운 모델이 필요할 때(예: 실시간 처리, 모바일 기기에서의 예측).


[임베딩]
    - 컴퓨터가 글자나 단어를 숫자로 바꾸는 방법이야. 이렇게 숫자로 바꾸면 컴퓨터가 단어의 뜻을 더 잘 이해하고 처리할 수 있어.
    -
[양방향RNN]
    - 데이터를 앞에서부터 읽을 뿐만 아니라 뒤에서부터도 읽는 특별한 신경망이야.
    - 보통 RNN은 문장을 앞에서부터 순서대로 읽으면서 다음에 나올 걸 예측하지만,
    양방향 RNN은 문장의 앞과 뒤를 모두 보고 예측을 해. 예를 들어,
    "나는 밥을 먹었다"라는 문장을 볼 때, 앞부분인 "나는"만 보지 않고, 뒤에 있는 "먹었다"도
    함께 보면서 더 정확하게 "밥"이라는 단어를 예측할 수 있어.

[스태킹RNN]
    - 여러 층으로 쌓인 순환신경망이야. 보통 RNN은 한 층으로만 데이터를 처리하지만,
    스태킹 RNN은 여러 층을 사용해서 더 복잡한 패턴을 배울 수 있어.
    - 예] 종이컵을 하나 쌓으면 높이가 낮지만, 여러 개를 쌓으면 더 높아지잖아?
    스태킹 RNN도 비슷해. 여러 층을 쌓아서 데이터 속에서 더 깊은 의미를 찾아내는 거야.

[순환 드롭아웃]
    - 컴퓨터가 순환신경망(RNN)을 학습할 때, 과적합이라는 문제를 막기 위해 사용되는 방법이야.
    과적합은 모델이 훈련 데이터에 너무 맞춰져서 새로운 데이터를 잘 처리하지 못하게 되는 걸 말해.
    - 드롭아웃은 RNN이 데이터를 처리하는 동안 일부 신경을 잠시 꺼버리는 것과 비슷해.
    예를 들어, 너가 시험 준비를 할 때 모든 문제를 다 공부하지 않고, 몇 가지 문제를 건너뛰며 공부하면 더 다양한 문제를 풀 수 있게 되는 것처럼,
    RNN도 데이터를 조금씩 건너뛰면서 학습해. 이렇게 하면 너무 한쪽에 치우친 학습을 피할 수 있어.

[NLP]










